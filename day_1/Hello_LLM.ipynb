{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.98.0)\n",
            "Collecting tiktoken (from -r requirements.txt (line 2))\n",
            "  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.4.1)\n",
            "Collecting regex>=2022.1.18 (from tiktoken->-r requirements.txt (line 2))\n",
            "  Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "Collecting requests>=2.26.0 (from tiktoken->-r requirements.txt (line 2))\n",
            "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2))\n",
            "  Using cached charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2))\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\usvidr03\\ai-llm-engineering-1\\.venv\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 1)) (0.4.6)\n",
            "Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
            "Downloading regex-2025.7.34-cp312-cp312-win_amd64.whl (275 kB)\n",
            "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: urllib3, regex, charset_normalizer, requests, tiktoken\n",
            "\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   ---------------------------------------- 0/5 [urllib3]\n",
            "   -------- ------------------------------- 1/5 [regex]\n",
            "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
            "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
            "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
            "   ------------------------ --------------- 3/5 [requests]\n",
            "   ------------------------ --------------- 3/5 [requests]\n",
            "   ------------------------ --------------- 3/5 [requests]\n",
            "   -------------------------------- ------- 4/5 [tiktoken]\n",
            "   ---------------------------------------- 5/5 [tiktoken]\n",
            "\n",
            "Successfully installed charset_normalizer-3.4.2 regex-2025.7.34 requests-2.32.4 tiktoken-0.9.0 urllib3-2.5.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzWFqd8r2KpZ6hbWwNa9XJqZBhS6g', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks designed to facilitate building applications that leverage large language models (LLMs), but they focus on different aspects and have distinct features:\\n\\n**LangChain:**\\n- **Purpose:** Primarily a framework for developing complex, composable language model applications, especially those involving chaining multiple steps, tools, APIs, and memory.\\n- **Features:**\\n  - Supports chaining multiple LLM calls with different prompts.\\n  - Facilitates integration with external tools, APIs, databases, and APIs.\\n  - Provides a modular architecture with components like prompt templates, memory, agents, and tools.\\n  - Useful for building conversational agents, question-answering systems, and applications requiring reasoning over multiple steps.\\n- **Use Cases:** Complex agent creation, autonomous agents, multi-step workflows, and integrations with other systems.\\n\\n**LlamaIndex (GPT Index):**\\n- **Purpose:** Focused on building indices over external data sources (like documents, databases, or APIs) to enable efficient retrieval and querying with LLMs.\\n- **Features:**\\n  - Helps create data structures (indices) over diverse data for fast retrieval.\\n  - Supports various index types (e.g., tree indices, list indices, hierarchical indices).\\n  - Simplifies the process of embedding and querying large document collections.\\n  - Optimized for building question-answering systems over external datasets.\\n- **Use Cases:** Document retrieval, knowledge base creation, question-answering over large datasets.\\n\\n**In summary:**\\n- **LangChain** excels at orchestrating multi-step interactions, integrating tools, and building complex language model workflows.\\n- **LlamaIndex** specializes in indexing external data sources to enable efficient retrieval and question-answering capabilities.\\n\\n**Both can be integrated together:** For example, using LlamaIndex to create a knowledge base, and LangChain to orchestrate interactions that query that index and perform additional processing.\\n\\n---\\n\\n**Note:** The landscape of AI frameworks evolves rapidly, so it's always good to check the latest documentation for updates.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754001694, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=412, prompt_tokens=19, total_tokens=431, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications with large language models (LLMs), but they have different focuses, architectures, and use cases. Here's an overview of their main differences:\n",
              "\n",
              "### 1. Purpose and Focus\n",
              "- **LangChain:**\n",
              "  - **Purpose:** Provides a flexible, modular framework to build complex, multi-step applications involving LLMs.\n",
              "  - **Focus:** Orchestrating language model interactions, including prompt management, chaining multiple calls, memory, and integrations with various data sources.\n",
              "  - **Use Cases:** Chatbots, question-answering pipelines, reasoning tasks, language model chaining, and custom workflows.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - **Purpose:** Designed primarily for creating index structures over external data sources to enable efficient retrieval and question-answering.\n",
              "  - **Focus:** Data ingestion, indexing, and retrieval from unstructured or semi-structured data (like documents, PDFs, websites) to build chatbots or AI tools that can answer questions based on specific datasets.\n",
              "  - **Use Cases:** Building semantic search, document-based QA systems, knowledge bases.\n",
              "\n",
              "### 2. Core Architecture\n",
              "- **LangChain:**\n",
              "  - Modular components for prompts, memory, chains, agents, and integrations.\n",
              "  - Supports a wide range of language models and APIs (OpenAI, Cohere, AI21, etc.).\n",
              "  - Emphasizes procedural workflows with chaining, conditionals, and reasoning.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focuses on data ingestion: creating indices (e.g., list index, tree index, GPT index) over datasets.\n",
              "  - Incorporates retrieval-augmented generation (RAG) techniques.\n",
              "  - Generally integrates with vector stores and embeddings for retrieval.\n",
              "\n",
              "### 3. Data Handling\n",
              "- **LangChain:**\n",
              "  - Handles user input and generates outputs; can connect to data sources but does not inherently offer data indexing.\n",
              "  - Often manages context and memory across interactions.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Specializes in processing large datasets, constructing indices, and retrieving relevant information to feed into LLMs for tasks like QA.\n",
              "  - Optimized for handling document collections and external knowledge bases.\n",
              "\n",
              "### 4. Community and Ecosystem\n",
              "- **LangChain:**\n",
              "  - Broader community with many integrations, examples, and tutorials.\n",
              "  - Active development focusing on flexibility and extensibility.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - More niche, focused on data ingestion and retrieval for LLM applications.\n",
              "  - Gaining popularity in applications needing document-based retrieval.\n",
              "\n",
              "### Summary Table\n",
              "\n",
              "| Aspect                     | LangChain                                     | LlamaIndex (GPT Index)                          |\n",
              "|----------------------------|----------------------------------------------|------------------------------------------------|\n",
              "| Primary Focus              | Workflow orchestration with LLMs             | Indexing and retrieval over external data   |\n",
              "| Use Cases                  | Chatbots, chaining, reasoning, agent frameworks | Document QA, semantic search, knowledge bases |\n",
              "| Data Handling              | Less focused on data ingestion; more on prompt management | Designed for processing and querying datasets |\n",
              "| Architecture               | Modular, chain-based workflows                | Index structures + retrieval mechanisms       |\n",
              "| Community & Ecosystem      | Larger, more general-purpose                   | Specialized, focused on data indexing        |\n",
              "\n",
              "---\n",
              "\n",
              "### In summary:\n",
              "- **Use LangChain** if you want to build complex, multi-step applications, workflows, or agents involving LLMs.\n",
              "- **Use LlamaIndex** if your goal is to ingest large collections of documents or data sources and perform efficient retrieval-based question-answering or search over that data.\n",
              "\n",
              "**Both can also be used together**: you might use LlamaIndex to index your data and LangChain to orchestrate complex interactions and workflows that incorporate retrieval results."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I can't believe you're even asking! After all this hunger and frustration, I just want ice that can give me some relief right now. Crushed ice all the way—nothing beats that icy crunch when you're starving and irate! Cubed ice? Please, that's slow and boring. Bring me crushed ice or nothing at all!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I love both! Crushed ice is perfect for making refreshing drinks like juleps and slushies, giving a cool and soothing feel. Cubed ice is great for keeping beverages cold without watering them down too quickly. If I had to choose, I might lean toward crushed ice for the fun, icy crunch—what about you?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzWIIs83hZETHuaCuBio3vYjjMXcB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I love both! Crushed ice is perfect for making refreshing drinks like juleps and slushies, giving a cool and soothing feel. Cubed ice is great for keeping beverages cold without watering them down too quickly. If I had to choose, I might lean toward crushed ice for the fun, icy crunch—what about you?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754001846, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=67, prompt_tokens=30, total_tokens=97, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to significant long-term shifts in temperature, weather patterns, and other atmospheric conditions on Earth. It is primarily driven by human activities such as the burning of fossil fuels, deforestation, and industrial processes, which increase greenhouse gas concentrations in the atmosphere. These changes lead to phenomena like rising sea levels, more frequent and severe storms, droughts, and changes in ecosystems. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect our planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay nako, mga ka-tropa! Alam nyo ba, ang climate change ay parang si baduy na nakakatawa kasi misteryo talaga eh. Parang nag-iinarte ang mundo natin—maging mainit, maging malamig, tapos nagluluhan na ako! Kanya-kanya na lang tayo ng anggulo, pero seryoso to, ha? Kung hindi tayo kikilos ngayon, baka maging “ice age” na tayo sa huli, tapos mapapailing na lang tayo. Kailangan natin maging mindful sa kalikasan—mag-recycle, mag-conserve ng energy, at wag gawing trash-bin ang planet. Kasi, sabi nga nila, “Huwag nang maghintay na lumamig pa ang daigdig bago tayo kumilos!” Kaya, tulong-tulungan tayo, mga beshie, para sa isang mas malamig, mas maaliwalas na mundo. Virtual hug to all!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench tightly secured the falbean in place, ensuring the machinery operated smoothly."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "In the Isekai series \"That Time I Got Reincarnated as a Slime,\" the protagonist is overpowered, effortlessly defeating enemies with his unparalleled abilities."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Assuming user_prompt and assistant_prompt functions are defined as before\n",
        "\n",
        "list_of_isekasi_prompts = [\n",
        "    # Teaching \"Isekai\" as a genre\n",
        "    user_prompt(\"In anime and manga, 'Isekai' (pronounced ee-seh-KAI, meaning 'different world') is a genre where a character is transported from their normal world to a new, often fantastical one. An example of a sentence describing an 'Isekai' story is:\"),\n",
        "    assistant_prompt(\"Many Isekai series feature protagonists who gain powerful abilities in their new world.\"),\n",
        "\n",
        "    # Asking for an example that includes a common Isekai trope\n",
        "    user_prompt(\"An 'overpowered protagonist' is a common trope in Isekai where the main character becomes incredibly strong. Can you give an example sentence about an Isekai series that has an overpowered protagonist?\")\n",
        "]\n",
        "\n",
        "isekai_response = get_response(client, list_of_isekasi_prompts)\n",
        "pretty_print(isekai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's carefully count the number of 'r's in the word \"strawberry\" step by step:\n",
              "\n",
              "1. Write down the word: **strawberry**\n",
              "2. Break it down into individual letters for clarity:\n",
              "\n",
              "   s t r a w b e r r y\n",
              "\n",
              "3. Identify and count each 'r':\n",
              "\n",
              "   - First 'r' appears in the 3rd position.\n",
              "   - Second 'r' appears in the 8th position.\n",
              "   - Third 'r' appears in the 9th position.\n",
              "\n",
              "4. Count the total number of 'r's:\n",
              "\n",
              "   There are 3 'r's in the word \"strawberry.\"\n",
              "\n",
              "**Final count:** \\(\\boxed{3}\\)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "instruction = \"\"\"\n",
        "You are a careful assistant. Count how many times the letter 'r' appears in the word \"strawberry\". \n",
        "Please list the steps and provide the final count.\n",
        "Answer: <number>\n",
        "\"\"\".strip()\n",
        "\n",
        "reasoning_problem = f\"\"\"\n",
        "How many r's in \"strawberry\"? {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
